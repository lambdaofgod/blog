
var documents = [{
    "id": 0,
    "url": "https://lambdaofgod.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://lambdaofgod.github.io/about/",
    "title": "About Me",
    "body": "This is where you put the contents of your About page. Like all your pages, it‚Äôs in Markdown format. This website is powered by fastpages 1.       a blogging platform that natively supports Jupyter notebooks in addition to other formats. ¬†&#8617;    "
    }, {
    "id": 2,
    "url": "https://lambdaofgod.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; ‚Ä¢ {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://lambdaofgod.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://lambdaofgod.github.io/2020/03/01/2020-03-01-Github_Starred_Repositories.html",
    "title": "",
    "body": "2020/03/01 -                 import reimport tqdmimport requestsimport numpy as npfrom markdown import markdownimport nltkimport pandas as pdfrom pandas. io. json import json_normalizefrom gensim import summarizationfrom sklearn import decomposition, feature_extraction, manifoldfrom sklearn. feature_extraction import stop_wordsfrom sklearn import pipelineimport rank_bm25import seaborn as snsimport wordcloudimport matplotlib. pyplot as pltfrom IPython. display import Markdown, displayimport bokeh. modelimport bokeh. plottingimport bokeh. ioimport reimport umapfrom sklearn import metricsimport holoviews as hvfrom holoviews import optsfrom holoviews. operation. datashader import datashade, dynspreadfrom holoviews. operation import decimateimport requestsfrom io import StringIObokeh. io. output_notebook()            Loading BokehJS . . .           plt. style. use(&#39;ggplot&#39;)          def printmd(string):  display(Markdown(string))  def get_word_cloud(texts):  text = &#39; &#39;. join(texts)  return wordcloud. WordCloud(max_font_size=40). generate(text)def show_word_cloud(wc, figure_kwargs={&#39;figsize&#39;: (8, 5)}):  plt. figure(**figure_kwargs)  plt. imshow(wc)  plt. axis(&#39;off&#39;)  plt. show() def show_word_cloud_from_texts(text_column):  texts = text_column. fillna(&#39;&#39;). values  cloud = get_word_cloud(texts)  show_word_cloud(cloud)    Note that I barely know GraphQL: I made this query in Github's API explorer       def run_query(query, key): # A simple function to use requests. post to make the API call. Note the json= section.   headers = {&#39;Authorization&#39;: &#39;token &#39; + key}  request = requests. post(&#39;https://api. github. com/graphql&#39;, json={&#39;query&#39;: query}, headers=headers)  if request. status_code == 200:    return request. json()  else:    raise Exception(&quot;Query failed to run by returning code of {}. {}&quot;. format(request. status_code, query))    # The GraphQL query (with a few aditional bits included) itself defined as a multi-line string.    def get_next_paged_result(result, key): if result is None:  end_cursor_string = &#39;&#39; else:  end_cursor_string = &#39;after: &quot;{}&quot;&#39;. format(result[&#39;data&#39;][&#39;viewer&#39;][&#39;starredRepositories&#39;][&#39;pageInfo&#39;][&#39;endCursor&#39;]) new_query_string = &quot;&quot;&quot;{{  viewer {{   starredRepositories(first: 100, {}) {{    pageInfo {{     startCursor     hasNextPage     endCursor    }}    nodes {{     name     owner {{      login     }}     description     primaryLanguage {{      name     }}     repositoryTopics(first: 10) {{      nodes {{       topic {{        name       }}      }}     }}     object(expression: &quot;master:README. md&quot;) {{      . . . on Blob {{       text      }}     }}    }}   }}  }} }}&quot;&quot;&quot;. format(end_cursor_string) return run_query(new_query_string, key)          def get_starred_repo_information(key, n_pages=6):  next_result = None  starred_repo_information = []  for __ in tqdm. tqdm(range(n_pages)):   next_result = get_next_paged_result(next_result, key)   starred_repo_information = starred_repo_information + next_result[&#39;data&#39;][&#39;viewer&#39;][&#39;starredRepositories&#39;][&#39;nodes&#39;]  return starred_repo_information    Put your Github token to github_auth_key. txt You have to do this because, unfortunately, for now there is no way to use GraphQL without authentication.       # key = open(&#39;github_auth_key. txt&#39;, &#39;r&#39;). read(). strip()# starred_repo_information = get_starred_repo_information()    Since this notebook is hosted on github pages, we'll use cached dataset       import picklestarred_repo_information = pickle. load(open(&#39;starred_repo_information. pkl&#39;, &#39;rb&#39;))           from nltk import stem, tokenizelemmatizer = stem. WordNetLemmatizer()lemmatizer. lemmatize(&#39;repositories&#39;)def clean_and_stem(text):  cleaned_text = re. sub(&#39;^[0-9a-zA-Z]+&#39;, &#39; &#39; , text. lower())  return &#39; &#39;. join([lemmatizer. lemmatize(w) for w in tokenize. wordpunct_tokenize(cleaned_text)])def get_cleaned_starred_repositories_df(repo_information):  repo_df = json_normalize(repo_information)  repo_df. index = repo_df[&#39;name&#39;]  repo_df. drop(&#39;name&#39;, axis=1, inplace=True)  repo_df[&#39;primaryLanguage&#39;] = repo_df[&#39;primaryLanguage. name&#39;]  repo_df. drop(&#39;primaryLanguage. name&#39;, axis=1)  repo_df[&#39;topics&#39;] = repo_df[&#39;repositoryTopics. nodes&#39;]. apply(lambda recs: [r[&#39;topic&#39;][&#39;name&#39;] for r in recs])  repo_df[&#39;topics&#39;] = repo_df[&#39;topics&#39;]. apply(lambda ts: [lemmatizer. lemmatize(t) for t in ts])  repo_df[&#39;description&#39;]. fillna(&#39;&#39;, inplace=True)  repo_df[&#39;description_stemmed&#39;] = repo_df[&#39;description&#39;]. apply(clean_and_stem)  repo_df[&#39;description_keywords&#39;] = repo_df[&#39;description_stemmed&#39;]. apply(summarization. keywords)  repo_df[&#39;description_length&#39;] = repo_df[&#39;description&#39;]. str. split(). apply(lambda l: 0 if l is None else len(l))  repo_df = repo_df[repo_df[&#39;description_length&#39;] &gt; 0]  return repo_df          def get_topic_representant_indices(topic_weights, topic_idx, num_representants=5):  indices = topic_weights[:, topic_idx]. argsort()[::-1]  return indices[:num_representants]def get_repos_representing_topic(repo_df, topic_weights, topic_idx, num_representants=5):  return repo_df. iloc[get_topic_representant_indices(topic_weights, topic_idx, num_representants)]          def plot_description_lengths(description_lengths):  hist, edges = np. histogram(description_lengths. values, bins=25)  median_description_length = description_lengths. median()  mean_description_length = description_lengths. mean()  p = bokeh. plotting. figure(    title=&#39;Description length&#39;,    x_axis_label=&#39;words in description&#39;,    y_axis_label=&#39;number of repositories&#39;,    plot_height=600, plot_width=800)  p. quad(top=hist, left=edges[:-1], right=edges[1:], bottom=0)  p. line([median_description_length, median_description_length], [0, 140], line_color=&#39;red&#39;)  bokeh. plotting. show(p)          from bokeh import palettesdef plot_2d_data(data, text_label, cls, show_text=True, subset=None):  palette = palettes. d3[&#39;Category20&#39;]  x, y = data[:, 0], data[:, 1]  source_df = pd. DataFrame({&#39;x&#39;: x, &#39;y&#39;: y, &#39;text_label&#39;: text_label, &#39;color&#39;: [palette[c + 3][c] for c in cls]})  source = bokeh. models. ColumnDataSource(source_df)  TOOLS=&quot;hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,&quot;    p = bokeh. plotting. figure(tools=TOOLS, plot_width=800, plot_height=600)  p. scatter(x=&#39;x&#39;, y=&#39;y&#39;, source=source, fill_color=&#39;color&#39;, line_color=&#39;color&#39;)  if subset is not None:    text_labels = bokeh. models. LabelSet(x=&#39;x&#39;, y=&#39;y&#39;, text=&#39;text_label&#39;, level=&#39;glyph&#39;,           x_offset=5, y_offset=5, source=bokeh. models. ColumnDataSource(source_df. iloc[subset]), render_mode=&#39;canvas&#39;, text_font_size=&#39;7pt&#39;)    p. add_layout(text_labels)  bokeh. plotting. show(p)          starred_repo_df = get_cleaned_starred_repositories_df(starred_repo_information)    Primary language       language_counts = starred_repo_df[&#39;primaryLanguage&#39;]. fillna(&#39;unspecified&#39;). value_counts()    Programming language&#182;      p = bokeh. plotting. figure(x_range=list(language_counts. index), title=&#39;Repository number by language&#39;)p. vbar(x=language_counts. index, top=language_counts, width=1)p. xaxis. major_label_orientation = &quot;vertical&quot;bokeh. plotting. show(p)           print(starred_repo_df[&#39;description_length&#39;]. describe())plot_description_lengths(starred_repo_df[&#39;description_length&#39;])  count  571. 000000mean   10. 082312std    6. 451048min    1. 00000025%    6. 00000050%    9. 00000075%    12. 000000max    69. 000000Name: description_length, dtype: float64         starred_repo_df = starred_repo_df[starred_repo_df[&#39;description_length&#39;] &gt; 5]    Topics word cloud&#182;      show_word_cloud_from_texts(starred_repo_df[&#39;topics&#39;]. apply(&#39; &#39;. join))    Descriptions word cloud       show_word_cloud_from_texts(starred_repo_df[&#39;description_stemmed&#39;])          show_word_cloud_from_texts(starred_repo_df[&#39;description_keywords&#39;])    Information retrieval crash course&#182;We have a collection of documents $d_i$ and want to find some documents. We formulate a query $q$ for which the system returns some documents with relevance scores. System can be evaluated (for queries with known responses) as a classifier. Because of that we use precision and recall scores (why these instead of accuracy?) Also we can use ranking metrics. Approaches&#182;: substring matching break down texts into word and match them Honorable mention - inverted index&#182;: The Vector Space Model&#182;: represent documents and queries as vectors use similarity/disssimilarity (distance) to score vectors for a query Bag of Words&#182;: TF-IDF, BM-25 can be interpreted as this - similarity is calculated as dot product in appropriate space sklearn. text. preprocessing. {Count|TfIdf}Vectorizer Now we can use machine learning!       import rank_bm25 class SearchEngine:    def __init__(self, df, bm25_cls=rank_bm25. BM25Okapi, text_col=&#39;text&#39;):    self. bm25 = bm25_cls(df[text_col]. str. split())    self. df = df      def search(self, query, k=100):    scores = self. bm25. get_scores(query. split())    #scores = scores[scores &gt; 0]    relevant_indices = np. argsort(-scores)[:k]    return self. df. iloc[relevant_indices[scores[relevant_indices] &gt; 0]]  search_engine = SearchEngine(starred_repo_df, text_col=&#39;description_stemmed&#39;)search_engine. search(&#39;information retrieval&#39;)           description   owner. login   primaryLanguage. name   repositoryTopics. nodes   object. text   primaryLanguage   object   topics   description_stemmed   description_keywords   description_length       name                                          musicinformationretrieval. com   Instructional notebooks on music information r. . .    stevetjoa   Jupyter Notebook   [{'topic': {'name': 'ipython-notebook'}}, {'to. . .    stanford-mir\n============\n\n[![Stories in Re. . .    Jupyter Notebook   NaN   [ipython-notebook, music-information-retrieval. . .    notebook on music information retrieval .       6       anserini   A Lucene toolkit for replicable information re. . .    castorini   Java   [{'topic': {'name': 'information-retrieval'}},. . .    Anserini\n========\n[![Build Status](https://t. . .    Java   NaN   [information-retrieval, lucene]   lucene toolkit for replicable information retr. . .    retrieval   8       awesome-information-retrieval   A curated list of awesome information retrieva. . .    harpribot   NaN   []   # Awesome Information Retrieval [![Awesome](ht. . .    NaN   NaN   []   curated list of awesome information retrieval . . .    retrieval   8       LIRE   Open source library for content based image re. . .    dermotte   Java   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    # LIRE - Lucene Image Retrieval\nLIRE (Lucene . . .    Java   NaN   [image-retrieval, lira, multimedia]   source library for content based image retriev. . .    retrieval   12       wikIR   A python tool for building large scale Wikiped. . .    getalp   Python   []   # WIKIR\nA python tool for building large scal. . .    Python   NaN   []   python tool for building large scale wikipedia. . .    retrieval\nlarge   11       pytrec_eval   pytrec_eval is an Information Retrieval evalua. . .    cvangysel   C++   [{'topic': {'name': 'information-retrieval'}},. . .    pytrec_eval\n===========\n\npytrec\_eval is a . . .    C++   NaN   [information-retrieval, evaluation]   _eval is an information retrieval evaluation t. . .    evaluation   14       query-expansion   Developing different methods for expanding a q. . .    phosseini   Python   []   # query-expansion\n\nThis repository is dedica. . .    Python   NaN   []   different method for expanding a query / topic. . .    expanding\nexpanded\nquery   19       cnnimageretrieval-pytorch   CNN Image Retrieval in PyTorch: Training and e. . .    filipradenovic   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    ## CNN Image Retrieval in PyTorch: Training an. . .    Python   NaN   [image-retrieval, convolutional-neural-network. . .    image retrieval in pytorch : training and eval. . .    retrieval   14       StarSpace   Learning embeddings for classification, retrie. . .    facebookresearch   C++   []   &lt;p align= center &gt;&lt;img width= 15%  src= exampl. . .    C++   NaN   []   embeddings for classification , retrieval and . . .       7       sparse_recovery   noiseless/nonnegative sparse recovery and feat. . .    NLPrinceton   Python   []   # sparse_recovery\n\nThis module provides solv. . .    Python   NaN   []   / nonnegative sparse recovery and feature retr. . .    sparse   9       revisitop   Revisiting Oxford and Paris: Large-Scale Image. . .    filipradenovic   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    # Revisiting Oxford and Paris: Large-Scale Ima. . .    Python   NaN   [image-retrieval, matlab, python]   oxford and paris : large - scale image retriev. . .    large   8       CBIR   üèû A content-based image retrieval (CBIR) system   pochih   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    [![Open Source Love](https://badges. frapsoft. c. . .    Python   NaN   [image-retrieval, computer-vision, gabor, hog,. . .    üèû a content - based image retrieval ( cbir ) s. . .    based   7       deep-image-retrieval   End-to-end learning of deep visual representat. . .    almazan   Python   []   # Deep Image Retrieval\n\nThis repository cont. . .    Python   NaN   []   - to - end learning of deep visual representat. . .    visual   9       minmaxcsa   MinMax Circular Sector Arc for External Plagia. . .    duartefellipe   Python   [{'topic': {'name': 'plagiarism-detection'}}, . . .    ## Minmax Circular Sector Arcs (MinMaxCSA): A . . .    Python   NaN   [plagiarism-detection, locality-sensitive-hash. . .    circular sector arc for external plagiarism ‚Äô . . .    sector   11       ir-python   A python implementation for information retrie. . .    zxzlogic   Python   []   # ir-python\nA python implementation for infor. . .    Python   NaN   []   python implementation for information retrieva. . .    retrieval\npython\nindex\nindexing\nsafe\ngoogle   34       ParetoMTL   Code for Neural Information Processing Systems. . .    Xi-L   Python   []   # Pareto Multi-Task Learning\nCode for Neural . . .    Python   NaN   []   for neural information processing system ( neu. . .    information   12       Open-IE-Papers   Open Information Extraction (OpenIE) and Open . . .    NPCai   NaN   [{'topic': {'name': 'openie'}}, {'topic': {'na. . .    # Table of Contents\n\n1. [General](#general)\. . .    NaN   NaN   [openie, literature-review, paper, nlp, inform. . .    information extraction ( openie ) and open rel. . .    extraction   12       hashing-baseline-for-image-retrieval   :octocat:Various hashing methods for image ret. . .    willard-yuan   MATLAB   [{'topic': {'name': 'hashing-library'}}, {'top. . .    # HABIR Toolkit\n\n[![License](https://img. shi. . .    MATLAB   NaN   [hashing-library, image-retrieval, ann]   : octocat : various hashing method for image r. . .       11       paws   This dataset contains 108,463 human-labeled an. . .    google-research-datasets   Python   []   # PAWS: Paraphrase Adversaries from Word Scram. . .    Python   NaN   []   dataset contains 108 , 463 human - labeled and. . .    labeled\nstructure   28       berkeley-doc-summarizer   The Berkeley Document Summarizer is a learning. . .    gregdurrett   Scala   []   berkeley-doc-summarizer\n=====================. . .    Scala   NaN   []   berkeley document summarizer is a learning - b. . .    document\nsyntactic\nbased   28     BM25 - a comment&#182;: BM comes from 'Best Match' Difference between TF-IDF: is not symmetrical (query and documents are treated in a different way, for example because their lengths tend to differ) Bag of Words&#182;Pros: can be very fasteasy to vectorizegood if you actually want to search by phraseCons: extremely high dimensionality - use sparse vectors or die (waiting or RAM)troubles with polysemous wordsvocabulary mismatch problem - synonymy      from sklearn import feature_extraction vectorizer = feature_extraction. text. TfidfVectorizer(ngram_range=(1,2))term_document_matrix = vectorizer. fit_transform(starred_repo_df[&#39;description_stemmed&#39;]) term_document_matrix. shape  (459, 5005)        search_engine. search(&#39;image&#39;)           description   owner. login   primaryLanguage. name   repositoryTopics. nodes   object. text   primaryLanguage   object   topics   description_stemmed   description_keywords   description_length       name                                          open-images   Build an example image classifier using Google. . .    quiltdata   Jupyter Notebook   []   # open images\n\nThis repository contains the . . .    Jupyter Notebook   NaN   []   an example image classifier using google open . . .    image   10       nsfw_data_source_urls   Collection of NSFW images URLs for the purpose. . .    EBazarov   NaN   []   # NSFW data source URLs\n\n## Description\n\nR. . .    NaN   NaN   []   of nsfw image url for the purpose of training . . .       14       cnnimageretrieval-pytorch   CNN Image Retrieval in PyTorch: Training and e. . .    filipradenovic   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    ## CNN Image Retrieval in PyTorch: Training an. . .    Python   NaN   [image-retrieval, convolutional-neural-network. . .    image retrieval in pytorch : training and eval. . .    retrieval   14       FUNIT_tensorflow   Tensorflow Implementation of FUNIT: Few-Shot U. . .    zhangqianhui   Python   [{'topic': {'name': 'image-to-image-translatio. . .    # FUNIT_tensorflow\nTensorflow Implementation . . .    Python   NaN   [image-to-image-translation, few-shot-learning. . .    implementation of funit : few - shot unsupervi. . .       8       imagehash   A Python Perceptual Image Hashing Module   JohannesBuchner   Python   []   NaN   Python   NaN   []   python perceptual image hashing module   perceptual   6       nsfw_data_scraper   Collection of scripts to aggregate image data . . .    alex000kim   Shell   [{'topic': {'name': 'nsfw-classifier'}}, {'top. . .    # NSFW Data Scraper\n\n## Note: use with cauti. . .    Shell   NaN   [nsfw-classifier, nsfw, deep-learning, content. . .    of script to aggregate image data for the purp. . .    image   16       image-to-image-papers   ü¶ì&lt;-&gt;ü¶í üåÉ&lt;-&gt;üåÜ A collection of image to image pa. . .    lzhbrian   NaN   [{'topic': {'name': 'image-to-image'}}, {'topi. . .    # Image-to-Image papers\n\nA collection of ima. . .    NaN   NaN   [image-to-image, generative-adversarial-networ. . .    ü¶ì&lt;-&gt;ü¶í üåÉ&lt;-&gt;üåÜ a collection of image to image pap. . .    constantly   13       snowy   Small Image Library for Python 3   prideout   Python   [{'topic': {'name': 'python'}}, {'topic': {'na. . .    [![Build Status](https://travis-ci. org/prideou. . .    Python   NaN   [python, image-processing]   small image library for python 3      6       image-match   üéá Quickly search over billions of images   EdjoLabs   Python   [{'topic': {'name': 'image-analysis'}}, {'topi. . .    [![PyPI](https://img. shields. io/pypi/status/im. . .    Python   NaN   [image-analysis, image-signatures, python, sea. . .    üéá quickly search over billion of image      7       revisitop   Revisiting Oxford and Paris: Large-Scale Image. . .    filipradenovic   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    # Revisiting Oxford and Paris: Large-Scale Ima. . .    Python   NaN   [image-retrieval, matlab, python]   oxford and paris : large - scale image retriev. . .    large   8       nsfw-v2   NSFW Image Detector with REST interface develo. . .    sajithm   Python   [{'topic': {'name': 'python'}}, {'topic': {'na. . .    # nsfw-v2\nAn NSFW detector serving responses . . .    Python   NaN   [python, nsfw-recognition, kera, flask, convol. . .    image detector with rest interface developed u. . .    interface   11       Image-to-Image-Search   A reverse image search engine powered by elast. . .    sethuiyer   Python   [{'topic': {'name': 'deep-learning'}}, {'topic. . .    &lt;img src= static/logo. jpg /&gt;\n\nSmartSearch is. . .    Python   NaN   [deep-learning, search-engine, elasticsearch, . . .    reverse image search engine powered by elastic. . .    search   11       MAX-Object-Detector   Localize and identify multiple objects in a s. . .    IBM   Python   [{'topic': {'name': 'docker-image'}}, {'topic'. . .    [![Build Status](https://travis-ci. com/IBM/MAX. . .    Python   NaN   [docker-image, machine-learning, machine-learn. . .    localize and identify multiple object in a sin. . .    multiple   9       imgdupes   Finding and deleting near-duplicate images bas. . .    knjcode   Python   [{'topic': {'name': 'image'}}, {'topic': {'nam. . .    # imgdupes\n\n`imgdupes` is a command line too. . .    Python   NaN   [image, dedupe, perceptual-hashing, perceptual. . .    and deleting near - duplicate image based on p. . .    image   9       albumentations   fast image augmentation library and easy to us. . .    albumentations-team   Python   [{'topic': {'name': 'image-augmentation'}}, {'. . .    # Albumentations\n[![PyPI version](https://bad. . .    Python   NaN   [image-augmentation, machine-learning, augment. . .    image augmentation library and easy to use wra. . .    augmentation   12       CBIR   üèû A content-based image retrieval (CBIR) system   pochih   Python   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    [![Open Source Love](https://badges. frapsoft. c. . .    Python   NaN   [image-retrieval, computer-vision, gabor, hog,. . .    üèû a content - based image retrieval ( cbir ) s. . .    based   7       deep-image-retrieval   End-to-end learning of deep visual representat. . .    almazan   Python   []   # Deep Image Retrieval\n\nThis repository cont. . .    Python   NaN   []   - to - end learning of deep visual representat. . .    visual   9       LIRE   Open source library for content based image re. . .    dermotte   Java   [{'topic': {'name': 'image-retrieval'}}, {'top. . .    # LIRE - Lucene Image Retrieval\nLIRE (Lucene . . .    Java   NaN   [image-retrieval, lira, multimedia]   source library for content based image retriev. . .    retrieval   12       DeOldify   A Deep Learning based project for colorizing a. . .    jantic   Jupyter Notebook   []   \n# DeOldify\n\nImage [&lt;img src= https://colab. . .    Jupyter Notebook   NaN   []   deep learning based project for colorizing and. . .    old   13       hashing-baseline-for-image-retrieval   :octocat:Various hashing methods for image ret. . .    willard-yuan   MATLAB   [{'topic': {'name': 'hashing-library'}}, {'top. . .    # HABIR Toolkit\n\n[![License](https://img. shi. . .    MATLAB   NaN   [hashing-library, image-retrieval, ann]   : octocat : various hashing method for image r. . .       11       dl-training-datasets   Set of scripts to download datasets of images . . .    SaMnCo   Shell   []   # About this repository\n\n**Notes 2016-01-08*. . .    Shell   NaN   []   of script to download datasets of image and cr. . .    annotation   17       colorization   Automatic colorization using deep neural netwo. . .    richzhang   Jupyter Notebook   [{'topic': {'name': 'caffe'}}, {'topic': {'nam. . .    &lt;!--&lt;h3&gt;&lt;b&gt;Colorful Image Colorization&lt;/b&gt;&lt;/h3. . .    Jupyter Notebook   NaN   [caffe, colorization, automatic-colorization, . . .    colorization using deep neural network .   col. . .    neural   12       image_captioning   Tensorflow implementation of  Show, Attend an. . .    DeepRNN   Python   []   ### Introduction\nThis neural system for image. . .    Python   NaN   []   implementation of   show , attend and tell : n. . .    caption   14       Colorizing-with-GANs   Grayscale Image Colorization with Generative A. . .    ImagingLab   Python   [{'topic': {'name': 'deep-learning'}}, {'topic. . .    # Image Colorization with Generative Adversari. . .    Python   NaN   [deep-learning, generative-adversarial-network. . .    image colorization with generative adversarial. . .    arxiv   8       tencent-ml-images   Largest multi-label image database; ResNet-101. . .    Tencent   Python   [{'topic': {'name': 'database'}}, {'topic': {'. . .    # Tencent ML-Images\n\nThis repository introdu. . .    Python   NaN   [database, deep-learning, computer-vision]   multi - label image database ; resnet - 101 mo. . .    label   11       DeepNude-an-Image-to-Image-technology   DeepNude's algorithm and general image generat. . .    yuanxiaosc   Python   [{'topic': {'name': 'image-to-image'}}, {'topi. . .    # DeepNude-an-Image-to-Image-technology\nGAN e. . .    Python   NaN   [image-to-image, pix2pix, cycle-gan, dcgan, st. . .    ' s algorithm and general image generation the. . .    general\ngeneration\npix\nmodel   22       pytorch-ssd   MobileNetV1, MobileNetV2, VGG based SSD/SSD-li. . .    qfgaohao   Python   [{'topic': {'name': 'ssd'}}, {'topic': {'name'. . .    # Single Shot MultiBox Detector Implementation. . .    Python   NaN   [ssd, pytorch, open-images, object-detection]   , mobilenetv2 , vgg based ssd / ssd - lite imp. . .    support\nssd\ndataset   28           search_engine. search(&#39;picture&#39;)           description   owner. login   primaryLanguage. name   repositoryTopics. nodes   object. text   primaryLanguage   object   topics   description_stemmed   description_keywords   description_length       name                                        Can we do anything with nonexact matches?&#182;: use different representation (word embeddings et c) change original representation Dimensionality reduction on text&#182;: Latent Semantic Indexing (sklearn. decomposition. TruncatedSVD) topic models Topic modeling (repo descriptions)&#182;: Idea - documents are probability distributions over vocabulary Model documents as mixtures of several latent factors This can be also considered as soft clustering (and turned into clustering by recovering the biggest component) I used LDA and NMF here, chose NMF because results looked better TL;DR NMF is a simpler model than LDA, probably better here since we have only ~500 examples&#182;: Nonnegative Matrix Factorization&#182;: Assume $t$ number of topics. Find nonnegative $L, T$ minimizing $\|\underset{n \times v}{D} - \underset{n \times t}{L}\ \underset{t \times D}{T}\|^2_F$ Similar to PCA (rank constraint) Can also add regularization       starred_repo_df. shape  (459, 11)        import ktrainnum_topics = 10 tm = ktrain. text. get_topic_model(  starred_repo_df[&#39;description_stemmed&#39;],  n_topics=num_topics,  model_type=&#39;nmf&#39;,  n_features=term_document_matrix. shape[1],  lda_max_iter=10,  min_df=1,  verbose=0,  hyperparam_kwargs={&#39;nmf_alpha&#39;: 0. 01, &#39;l1_ratio&#39;: 0. 5, &#39;ngram_range&#39;: (1,1)})  using Keras version: 2. 2. 4-tf        tm. build(starred_repo_df[&#39;description_stemmed&#39;])          tm. print_topics(show_counts=True)  topic:2 | count:50 | python using module retrieval http tool algorithm including leveldb implementationtopic:4 | count:33 | text summarization model using evaluation document extractive abstractive deep frameworktopic:8 | count:32 | network neural paper list curated code shot zero repository resourcetopic:0 | count:29 | learning deep machine shot book interactive scalable source representation modeltopic:9 | count:27 | data library structure topological science manifold graph point neighborhood notebooktopic:3 | count:26 | language processing natural nlp polish art course state list datasetstopic:6 | count:22 | image pytorch retrieval implementation based nsfw open training classifier informationtopic:5 | count:15 | search com vector talk semantic expansion engine query work buildtopic:7 | count:13 | library machine support causal tree framework regression classification gradient inferencetopic:1 | count:3 | model task code training semantic similarity achieve paper sentence result        reduced_term_document_matrix = tm. predict(starred_repo_df[&#39;description_stemmed&#39;])representative_repos = [get_repos_representing_topic(starred_repo_df, reduced_term_document_matrix, topic)[[&#39;description_stemmed&#39;]] for topic in range(num_topics)]    Topic keywords and most representative repositories&#182;:       topic_words = tm. get_topics()for topic in range(num_topics):  printmd(&quot;&quot;&quot;------\n# Topic {}\n------&quot;&quot;&quot;. format(topic+1))  show_word_cloud_from_texts(representative_repos[topic][&#39;description_stemmed&#39;])  printmd(&#39;# Keywords&#39;)  display(set(topic_words[topic]. split()))  printmd(&#39;## **repositories representative for {}th topic:**&#39;. format(topic + 1))  display(representative_repos[topic])  print()  Topic 1&#182;Keywords&#182;{&#39;book&#39;, &#39;deep&#39;, &#39;interactive&#39;, &#39;learning&#39;, &#39;machine&#39;, &#39;model&#39;, &#39;representation&#39;, &#39;scalable&#39;, &#39;shot&#39;, &#39;source&#39;}repositories representative for 1th topic:&#182;:          description_stemmed       name            h2o-3   source fast scalable machine learning platform. . .        vowpal_wabbit   wabbit is a machine learning system which push. . .        mxnet-the-straight-dope   interactive book on deep learning . much easy . . .        LearningToCompare_ZSL   code for cvpr 2018 paper : learning to compare. . .        d2l-en   into deep learning : an interactive deep learn. . .    Topic 2&#182;Keywords&#182;{&#39;achieve&#39;, &#39;code&#39;, &#39;model&#39;, &#39;paper&#39;, &#39;result&#39;, &#39;semantic&#39;, &#39;sentence&#39;, &#39;similarity&#39;, &#39;task&#39;, &#39;training&#39;}repositories representative for 2th topic:&#182;:          description_stemmed       name            iclr2016   code for training all model in the iclr paper . . .        ir-python   python implementation for information retrieva. . .        anchor-baggage   code for the article   building topic model ba. . .        multifit   code to reproduce result from paper   multifit. . .        sentence-similarity   implementation of various deep learning model . . .    Topic 3&#182;Keywords&#182;{&#39;algorithm&#39;, &#39;http&#39;, &#39;implementation&#39;, &#39;including&#39;, &#39;leveldb&#39;, &#39;module&#39;, &#39;python&#39;, &#39;retrieval&#39;, &#39;tool&#39;, &#39;using&#39;}repositories representative for 3th topic:&#182;:          description_stemmed       name            ir-python   python implementation for information retrieva. . .        data-science-ipython-notebooks   science python notebook : deep learning ( tens. . .        boilerpipe3   fork of boilerpipe with python 3 and small fix. . .        gputil   python module for getting the gpu status from . . .        xlearn   performance , easy - to - use , and scalable m. . .    Topic 4&#182;Keywords&#182;{&#39;art&#39;, &#39;course&#39;, &#39;datasets&#39;, &#39;language&#39;, &#39;list&#39;, &#39;natural&#39;, &#39;nlp&#39;, &#39;polish&#39;, &#39;processing&#39;, &#39;state&#39;}repositories representative for 4th topic:&#182;:          description_stemmed       name            NLP-progress   to track the progress in natural language proc. . .        polish-nlp-resources   - trained model and language resource for natu. . .        Introduction-to-Natural-Language-Processing-UMich-Coursera   repository contains weekly assignment on imple. . .        flair   very simple framework for state - of - the - a. . .        nlp-datasets   list of free / public domain datasets with tex. . .    Topic 5&#182;Keywords&#182;{&#39;abstractive&#39;, &#39;deep&#39;, &#39;document&#39;, &#39;evaluation&#39;, &#39;extractive&#39;, &#39;framework&#39;, &#39;model&#39;, &#39;summarization&#39;, &#39;text&#39;, &#39;using&#39;}repositories representative for 5th topic:&#182;:          description_stemmed       name            tf-textanalysis-gcp   how to perform text preprocessing using bigque. . .        nnsum   extractive neural network text summarization l. . .        jann   . i am jann . i am text input - text output ch. . .        Kashgari   is a production - ready nlp transfer learning . . .        python-sirajnet   deep complicated nlp to turn your text into my. . .    Topic 6&#182;Keywords&#182;{&#39;build&#39;, &#39;com&#39;, &#39;engine&#39;, &#39;expansion&#39;, &#39;query&#39;, &#39;search&#39;, &#39;semantic&#39;, &#39;talk&#39;, &#39;vector&#39;, &#39;work&#39;}repositories representative for 6th topic:&#182;:          description_stemmed       name            VectorsInSearch   . com repo to accompany the dice . com ' vecto. . .        columbiau-rocchio-search-query-expander   rocchio query expansion - similar to   related. . .        Kaggle_CrowdFlower   place solution for search result relevance com. . .        gnes   is generic neural elastic search , a cloud - n. . .        Image-to-Image-Search   reverse image search engine powered by elastic. . .    Topic 7&#182;Keywords&#182;{&#39;based&#39;, &#39;classifier&#39;, &#39;image&#39;, &#39;implementation&#39;, &#39;information&#39;, &#39;nsfw&#39;, &#39;open&#39;, &#39;pytorch&#39;, &#39;retrieval&#39;, &#39;training&#39;}repositories representative for 7th topic:&#182;:          description_stemmed       name            cnnimageretrieval-pytorch   image retrieval in pytorch : training and eval. . .        pytorch-ssd   , mobilenetv2 , vgg based ssd / ssd - lite imp. . .        nsfw_data_source_urls   of nsfw image url for the purpose of training . . .        nsfw_data_scraper   of script to aggregate image data for the purp. . .        FUNIT_tensorflow   implementation of funit : few - shot unsupervi. . .    Topic 8&#182;Keywords&#182;{&#39;causal&#39;, &#39;classification&#39;, &#39;framework&#39;, &#39;gradient&#39;, &#39;inference&#39;, &#39;library&#39;, &#39;machine&#39;, &#39;regression&#39;, &#39;support&#39;, &#39;tree&#39;}repositories representative for 8th topic:&#182;:          description_stemmed       name            adversarial-robustness-toolbox   library for adversarial machine learning ( eva. . .        catboost   fast , scalable , high performance gradient bo. . .        simpletransformers   made simple with training , evaluation , and p. . .        dowhy   is a python library for causal inference that . . .        nmslib   - metric space library ( nmslib ): an efficien. . .    Topic 9&#182;Keywords&#182;{&#39;code&#39;, &#39;curated&#39;, &#39;list&#39;, &#39;network&#39;, &#39;neural&#39;, &#39;paper&#39;, &#39;repository&#39;, &#39;resource&#39;, &#39;shot&#39;, &#39;zero&#39;}repositories representative for 9th topic:&#182;:          description_stemmed       name            ZeroShotCapsule   for paper   zero - shot user intent detection . . .        distiller   network distiller by intel ai lab : a python p. . .        awesome-rnn   neural network - a curated list of resource de. . .        LearningToCompare_ZSL   code for cvpr 2018 paper : learning to compare. . .        Inhibited-softmax   with code for paper   inhibited softmax for un. . .    Topic 10&#182;Keywords&#182;{&#39;data&#39;, &#39;graph&#39;, &#39;library&#39;, &#39;manifold&#39;, &#39;neighborhood&#39;, &#39;notebook&#39;, &#39;point&#39;, &#39;science&#39;, &#39;structure&#39;, &#39;topological&#39;}repositories representative for 10th topic:&#182;:          description_stemmed       name            topopy   library for computing topological data structu. . .        dagster   python library for building data application :. . .        data-science-ipython-notebooks   science python notebook : deep learning ( tens. . .        industry-machine-learning   curated list of applied machine learning and d. . .        ttk   - topological data analysis and visualization . . .            tm. train_recommender(n_neighbors=3, metric=&#39;cosine&#39;)          def show_results(query):  for res in tm. recommend(query, n=5, n_neighbors=3):    print(res[0])    print()show_results(&#39;search&#39;)  and memory - efficient ann with a subset - search functionalitysimple elasticsearch plugin wrapping around the search endpoint to provide rocchio query expansionquery expansion in semantic meta - search engine . the resulting expansion system is called wiki - metasemantik . search engine with query expansion. com repo to accompany the dice . com &#39; vector in search &#39; talk by simon hughes , from the activate 2018 search conference , and the &#39; searching with vector &#39; talk from haystack 2019 ( u ). build upon my conceptual search and semantic search work from 2015        show_results(&#39;query&#39;)  and memory - efficient ann with a subset - search functionalitysimple elasticsearch plugin wrapping around the search endpoint to provide rocchio query expansionrocchio query expansion - similar to &#34; related search :&#34; found at popular search engine but based on relevant document selected by the end - usersearch engine with query expansion. com repo to accompany the dice . com &#39; vector in search &#39; talk by simon hughes , from the activate 2018 search conference , and the &#39; searching with vector &#39; talk from haystack 2019 ( u ). build upon my conceptual search and semantic search work from 2015        show_results(&#39;information retrieval&#39;)  &#39; s algorithm and general image generation theory and practice research , including pix2pix , cyclegan , ugatit , dcgan , singan and vae model ( tensorflow2 implementation ). deepnudeÁöÑÁÆóÊ≥ï‰ª•ÂèäÈÄöÁî®ganÂõæÂÉèÁîüÊàêÁöÑÁêÜËÆ∫‰∏éÂÆûË∑µÁ†îÁ©∂ „ÄÇimage detector with rest interface developed using kera and flaskimage retrieval in pytorch : training and evaluating cnns for image retrieval in pytorchoxford and paris : large - scale image retrieval benchmarking: octocat : various hashing method for image retrieval and serf a the baseline  Visualizing repository 2D projection&#182;: Remark: ktrain also has visualization capability but I liked UMAP better       umap_red = umap. UMAP(metric=&#39;precomputed&#39;) umap_features = umap_red. fit_transform(metrics. pairwise. cosine_distances(reduced_term_document_matrix, reduced_term_document_matrix))          representatives = pd. concat(representative_repos)          representative_indices = np. where(starred_repo_df. index. isin(representatives. index))          umap_df = pd. DataFrame(umap_features)umap_df. columns = [&#39;x&#39;, &#39;y&#39;]umap_df[&#39;name&#39;] = starred_repo_df. indexumap_df[&#39;topic&#39;] = np. argmax(reduced_term_document_matrix, axis=1)          hv. notebook_extension(&#39;bokeh&#39;,&#39;matplotlib&#39;)opts. defaults(  opts. RGB(width=400, height=400, xaxis=None, yaxis=None, show_grid=False, bgcolor=&quot;black&quot;))points = hv. Points(umap_df)labels = hv. Labels(umap_df, [&#39;x&#39;,&#39;y&#39;], &#39;name&#39;)points. opts(  opts. Points(    color=&#39;topic&#39;,    cmap=&#39;Category20&#39;,    tools=[&#39;zoom_in&#39;, &#39;zoom_out&#39;, &#39;hover&#39;], width=800, height=600),  opts. Overlay(width=800, height=600),)  &lt;/img&gt; &lt;/img&gt;  &lt;/img&gt;    More&#182;my github profile - lambdaofgod GitHub GraphQL API Information Retrieval book LDA, Sparse Coding, Matrix Factorization and All Thatktrain on githubBM25UMAP"
    }, {
    "id": 5,
    "url": "https://lambdaofgod.github.io/fastpages/jupyter/2020/02/25/Sparsity_Presentation.html",
    "title": "Sparse linear models for image denoising",
    "body": "2020/02/25 -           What will be covered&#182;: Introduce sparse methods and concept of sparsity Mention few contexts and approaches Show an example of simple sparse method in action (image denoising) Advanced topics - relationship with other sparse methods, and Maximum A Posteriori estimation Teaser&#182;: Heard about Dictionary Learning? What is sklearn. decomposition. DictionaryLearning?Seen Orthogonal Matching Pursuit? What is sklearn. linear_model. OrthogonalMatchingPursuit?JPEG algorithmin what sense does LASSO (L1 regularization) induce sparsity?What are sparse methods?&#182;: Sparse methods in signal processing use special structure that is shared by some signals, that enables representing them as linear combinations of base signals ('atoms') with few nonzeros or big values. Example applications: Image denoising&#182;: Source Separation/inpainting&#182;: Simultaneous cartoon and texture image inpainting usingmorphological component analysis (MCA) Other applications superresolution (AKA image upsampling)image compressionstyle transferLinear equations and sparsity&#182;: Consider linear system $Ax = y$. For now we'll assume that this system has solution, but it might have many such solutions. Sparse methods aim at finding canonical solution where there might be many potential guesses - the original problem becomes Find $x$ such that $Ax = y$, and $x$ is sparse. How to define sparsity precisely?&#182;: In math/signal processing several types of metrics are used to measure sparsity. $L_0$ and $L_1$&#182;: Sparsity is most commonly measured using $L_0$ metric or $L_1$ 'metric'. $L_1$ metric, also known as Manhattan distance, is defined as $\|x\|_1 = \sum_{i}{|x_i|}$ Whereas $L_0$ is not actually a metric, and is defined as $\|x\|_0 = supp(x) =$ number of nonzero coefficients in $x$ Why use two separate notions? Answer: $L_0$ is not even continuous, and it is hard to optimize. In general leads to NP-hard optimization problem! $L_1$ on the other hand is continuous and convex. Also, $L_1$ can be thought of as a relaxation (convexification) of $L_0$: In fact, finding $x$ such that $Ax = b$ , $\|x\|_1$ minimal can be solved with linear program. L1 vs L2 (Euclidean) norm&#182;: A person familiar with regularization in linear regression might ask: What's the difference between minimizing $\|x\|^2$ and $\|x\|_1$ while solving $Ax = b$? Doesn't minimizing Euclidean norm lead to small coefficients? The problem: structure Comment on sparse data structures&#182;: Sparsity in mathematics/signal processing is related, but not equal to sparsity from computer science point of view. In computer science sparse matrix is a data structure that holds only nonzero coefficients. The relation between sparse data structure and mathematial notion lies in fact that sparse matrix data structure works well for representing matrices sparse in precise sense - this is also actually used in many 'sparse algorithms'. Dictionary learning and related terminology&#182;: Convention: $x \approx D\alpha$ where $\alpha$ is found with some kind of sparse method. The $D$ matrix is called dictionary. $\alpha$ gives $x$'s decomposition into atoms. Examples of dictionaries: Fourier transforms (JPEG uses Discrete Cosine Transform)Wavelet transforms$D$ might be also learned from training data, which is the task of dictionary learning. Denoising problem formulation&#182;: In denoising most commonly we pose problem as $y = x_{noisy} = x + \epsilon$ where $\epsilon$ has iid. normally distributed coefficients with zero mean and std. dev $\sigma$. Sparse models for denoising&#182;: Using our terminology we pose the problem as Minimize $\|x - D\alpha\|^2$ with $\|\alpha\|_0 \leq k$ Optimization algorithm&#182;: One of the simplest algorithms for solving such problems is thresholding algorithm (here we assume $D$ is column-normalized):       def thresholding(D, x, t):  coeffs = D. T @ x  mask = np. abs(coeffs) &gt; t  return coeffs * mask    Note that this algorithm naturally enforces sparsity - as t increases, it can only zero out more coefficients. Used dictionary&#182;: For this part we will use Daubechies wavelets coefficients. How can it be extended&#182;: Use different wavelet transform Use patch-based processing, and then average the patches (sklearn. feature_extraction. image. extract_patches_2d) Use different algorithm (these methods are called pursuit algorithms), for example Orthogonal Matching Pursuit or Basis Pursuit Dictionary learning: adapt (note that this would require multiple images, or image patches) scikit-learn: Image denoising using dictionary learning¬∂  Advanced topics&#182;: Other method that could utilize sparsity&#182;: Sparsity naturally comes up in matrix factorization (from using some form of L1 regularization) The general form of matrix factorization $argmin_{A, X} \rho(Y, AX) + \lambda \phi(A) + \gamma \psi(X)$ $\rho, \psi, \phi$ are most commonly some matrix norms. Example: PCA solves problem with $\rho(Y, AX) = \|Y - AX\|^2_F, \phi = \psi = 0$ Nonnegative Matrix Factorization Sparse PCA (like PCA, but $\psi(X) = \sum_{i}{\|X_i\|_1}$) Robust PCA ($\psi(X) = \sum_{i}{\|X_i\|_1}$, $\phi(A) = \|A\|_{*} = tr(\sqrt{A^T A}) = $ sum of $A$'s singular values) Sparse autoencoders Maximum A Posteriori estimation&#182;: A precise way to formulate the problem is to pose it as Maximum A Posteriori estimation: Recall that Maximum A Posteriori estimate of parameters $X$ given data $Y$ is defined as maximizing $$\hat{X} = argmax_{X}P(X | Y)$$Thus expression analogous to negative log-likelihood becomes (with Gaussian noise) $$-logP(X | Y) \propto$$$$ -log(P(Y|X)P(X)) \propto$$ $$ -log(e^{\|Y - DX\|^2} e^{\lambda\|X\|_1}) =$$ $$ \|Y - DX\|^2 + \lambda\|X\|_1$$ Resources&#182;: The go-to person for topics on sparse models in image &amp; signal processing is Michael Elad, author of: Five Lectures on Sparse and Redundant Representations - an introductory article Sparse and Redundant Representations edX Specialization For statistical learning point of view: Statistical Learning with Sparsity book - its authors were working with people who came up with these ideas in the first place In context of signal processing these topics are also jointly called compressed sensing. People who are working in this branch tend to work on random matrix theory, for example Emanuel Candes and Terence Tao. "
    }, {
    "id": 6,
    "url": "https://lambdaofgod.github.io/fastpages/jupyter/2020/02/20/test.html",
    "title": "Fastpages Notebook Blog Post",
    "body": "2020/02/20 -           About&#182;This notebook is a demonstration of some of capabilities of fastpages with notebooks. With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! Front Matter&#182;: Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: Setting toc: true will automatically generate a table of contentsSetting badges: true will automatically include GitHub and Google Colab links to your notebook. Setting comments: true will enable commenting on your blog post, powered by utterances. More details and options for front matter can be viewed on the front matter section of the README. Markdown Shortcuts&#182;: put a #hide flag at the top of any cell you want to completely hide in the docs put a #collapse flag at the top of any cell if you want to hide that cell by default, but stil have it be visible to the reader:              #collapseimport pandas as pdimport altair as alt       put a #collapse_show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:              #collapse_showcars = &#39;https://vega. github. io/vega-datasets/data/cars. json&#39;movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;sp500 = &#39;https://vega. github. io/vega-datasets/data/sp500. csv&#39;stocks = &#39;https://vega. github. io/vega-datasets/data/stocks. csv&#39;flights = &#39;https://vega. github. io/vega-datasets/data/flights-5k. json&#39;       Interactive Charts With Altair&#182;: Charts made with Altair remain interactive.  Example charts taken from this repo, specifically this notebook. Example 1: DropDown&#182;:       # single-value selection over [Major_Genre, MPAA_Rating] pairs# use specific hard-wired values as the initial selected valuesselection = alt. selection_single(  name=&#39;Select&#39;,  fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;],  init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;},  bind={&#39;Major_Genre&#39;: alt. binding_select(options=genres), &#39;MPAA_Rating&#39;: alt. binding_radio(options=mpaa)}) # scatter plot, modify opacity based on selectionalt. Chart(movies). mark_circle(). add_selection(  selection). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=&#39;IMDB_Rating:Q&#39;,  tooltip=&#39;Title:N&#39;,  opacity=alt. condition(selection, alt. value(0. 75), alt. value(0. 05)))    Example 2: Tooltips&#182;:       alt. Chart(movies). mark_circle(). add_selection(  alt. selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;])). encode(  x=&#39;Rotten_Tomatoes_Rating:Q&#39;,  y=alt. Y(&#39;IMDB_Rating:Q&#39;, axis=alt. Axis(minExtent=30)), # use min extent to stabilize axis title placement  tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;]). properties(  width=600,  height=400)    Example 3: More Tooltips&#182;:       # select a point for which to provide details-on-demandlabel = alt. selection_single(  encodings=[&#39;x&#39;], # limit selection to x-axis value  on=&#39;mouseover&#39;, # select on mouseover events  nearest=True,  # select data point nearest the cursor  empty=&#39;none&#39;   # empty selection includes no data points)# define our base line chart of stock pricesbase = alt. Chart(). mark_line(). encode(  alt. X(&#39;date:T&#39;),  alt. Y(&#39;price:Q&#39;, scale=alt. Scale(type=&#39;log&#39;)),  alt. Color(&#39;symbol:N&#39;))alt. layer(  base, # base line chart    # add a rule mark to serve as a guide line  alt. Chart(). mark_rule(color=&#39;#aaa&#39;). encode(    x=&#39;date:T&#39;  ). transform_filter(label),    # add circle marks for selected time points, hide unselected points  base. mark_circle(). encode(    opacity=alt. condition(label, alt. value(1), alt. value(0))  ). add_selection(label),  # add white stroked text to provide a legible background for labels  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),  # add text labels for stock prices  base. mark_text(align=&#39;left&#39;, dx=5, dy=-5). encode(    text=&#39;price:Q&#39;  ). transform_filter(label),    data=stocks). properties(  width=700,  height=400)    Data Tables&#182;: You can display tables per the usual way in your blog:       movies = &#39;https://vega. github. io/vega-datasets/data/movies. json&#39;df = pd. read_json(movies)# display table with pandasdf[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;,   &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]]. head()           Title   Worldwide_Gross   Production_Budget   IMDB_Rating         0   The Land Girls   146083. 0   8000000. 0   6. 1       1   First Love, Last Rites   10876. 0   300000. 0   6. 9       2   I Married a Strange Person   203134. 0   250000. 0   6. 8       3   Let's Talk About Sex   373615. 0   300000. 0   NaN       4   Slam   1087521. 0   1000000. 0   3. 4     Images&#182;: Local Images&#182;: You can reference local images and they will be copied and rendered on your blog automatically.  You can include these with the following markdown syntax: ![](my_icons/fastai_logo. png) Remote Images&#182;: Remote images can be included with the following markdown syntax: ![](https://image. flaticon. com/icons/svg/36/36686. svg) Animated Gifs&#182;: Animated Gifs work, too! ![](https://upload. wikimedia. org/wikipedia/commons/7/71/ChessPawnSpecialMoves. gif) Captions&#182;: You can include captions with markdown images like this: ![](https://www. fast. ai/images/fastai_paper/show_batch. png  Credit: https://www. fast. ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/ ) Other Elements&#182;Tweetcards&#182;: Typing &gt; twitter: https://twitter. com/jakevdp/status/1204765621767901185?s=20 will render this:Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Youtube Videos&#182;: Typing &gt; youtube: https://youtu. be/XfoYk_Z5AkI will render this: Boxes / Callouts&#182;: Typing &gt; Warning: There will be no second warning! will render this:    Warning: There will be no second warning! Typing &gt; Important: Pay attention! It's important. will render this:    Important: Pay attention! It&#8217;s important. Typing &gt; Tip: This is my tip. will render this:    Tip: This is my tip. Typing &gt; Note: Take note of this. will render this:    Note: Take note of this. Typing &gt; Note: A doc link to [an example website: fast. ai](https://www. fast. ai/) should also work fine. will render in the docs:    Note: A doc link to an example website: fast. ai should also work fine. "
    }, {
    "id": 7,
    "url": "https://lambdaofgod.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
    "title": "Example Markdown Post",
    "body": "2020/01/14 - Basic setup: Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-filename. md Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. . md is the file extension for markdown files. The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. Basic formatting: You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: Lists: Here‚Äôs a list:  item 1 item 2And a numbered list:  item 1 item 2Boxes and stuff:  This is a quotation    You can include alert boxes‚Ä¶and‚Ä¶    You can include info boxesImages: Code: General preformatted text: # Do a thingdo_thing()Python code and output: # Prints '2'print(1+1)2Tables:       Column 1   Column 2         A thing   Another thing   Tweetcards: Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019Footnotes:       This is the footnote. ¬†&#8617;    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}